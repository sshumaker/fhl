---
layout: handbook-page-toc
title: "Reliability Engineering"
---

## On this page
{:.no_toc .hidden-md .hidden-lg}

- TOC
{:toc .hidden-md .hidden-lg}

If you are a GitLab team member and are looking to alert Reliability Engineering about an availability issue with GitLab.com, please find quick instructions to report an incident here: [Reporting an Incident](/handbook/engineering/infrastructure/incident-management/#reporting-an-incident).
{: .alert .alert-danger}

If you are a GitLab team member looking for assistance from Reliability Engineering, please see the [Getting Assistance](#getting-assistance) section.
{: .alert .alert-info}

## Who We Are

Reliability Engineering is responsible for all of GitLab's user-facing services, with their primary responsibility being GitLab.com. Site Reliability Engineers (SREs) ensure the availability of these services, building the tools and automation to monitor and enable this availability. These user-facing services include a multitude of environments, including staging, GitLab.com, and dev.GitLab.org, among others (see the [list of environments](/handbook/engineering/infrastructure/environments/)).

## Vision

**Reliability Engineering** ensures that GitLab's customers can rely on GitLab.com for their mission-critical workloads. We approach availability as an engineering challenge and empower our counterparts in Development to make the best possible infrastructure decisions. We own and iterate often on [how we manage incidents](/handbook/engineering/infrastructure/incident-management/) and continually derive and share our learnings by conducting [thorough reviews of those incidents](/handbook/engineering/infrastructure/incident-review/).

## Getting Assistance

If you're a GitLab team member and are looking to alert Reliability Engineering about an availability issue with GitLab.com, please find quick instructions to report an incident here: [Reporting an Incident](/handbook/engineering/infrastructure/incident-management/#reporting-an-incident).
{: .alert .alert-danger}

If you'd like our assistance, please use one of the issue generation templates below and the work will be routed appropriately:

* [Open a General Request Issue](https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/new?issuable_template=default) - follow this link to create a general issue for the Reliability Team.

We can also be reached in Slack in the [#production](https://gitlab.slack.com/archives/C101F3796) channel for questions related to GitLab.com and in the [#infrastructure-lounge](https://gitlab.slack.com/archives/CB3LSMEJV) channel for all other questions.

### External Customer Escalations

Assistance from the Infrastructure Team is occasionally required to help solve or troubleshoot external customer issues.

1. Reach out to the [Support Team](/handbook/support/) to ensure that they are aware of the external customer issue.
2. File an issue in the [Reliability Issue Tracker](https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues/new?issuable_template=default)
3. Ping @reliability-ems on Slack in the #reliability-lounge channel and include a link to the newly created issue.

## Deploying changes to GitLab.com

All larger features, configuration changes, and new services must go through a [Production Readiness] review.

## Tenets

1. [**Change Management**](/handbook/engineering/infrastructure/change-management/), [**Incident Management**](/handbook/engineering/infrastructure/incident-management/), and [**Incident Review**](/handbook/engineering/infrastructure/incident-review/) are owned by Reliability Engineering.
1. Each team member is able to perform oncall for all services.
1. The team is able to reach conclusions independently all the time, consensus most of the time.
1. Career development paths are clear.
1. The team maintains a database of SRE knowledge through documentation, training sessions, and outreach.
1. We leverage the GitLab product where we can in our toolchain.


## On-Call

 * Manage alerts and incidents as described on the [Incident Management Page](/handbook/engineering/infrastructure/incident-management/#engineer-on-call-eoc-responsibilities)

## Reliability Teams

### General

The [General Team](/handbook/engineering/infrastructure/team/reliability/general.html) supports the Reliability Team's [overall vision](/handbook/engineering/infrastructure/team/reliability/#vision) by supporting services for GitLab.com that do not fit the mission of the other [Reliability Teams](/handbook/engineering/infrastructure/team/reliability/#reliability-teams).

### Observability

The [Observability team](/handbook/engineering/infrastructure/team/reliability/observability.html) maintains metrics and logs platforms for GitLab SaaS and is responsible for Prometheus, Thanos, Grafana, and [Logging](https://gitlab.com/gitlab-com/runbooks/-/tree/master/docs/logging).

### Foundations

The [Foundations team](/handbook/engineering/infrastructure/team/reliability/foundations.html) builds, runs and owns the core infrastructure for GitLab.com.

### Database Reliability

The [Database Reliability Team](/handbook/engineering/infrastructure/team/reliability/database-reliability.html) is responsible for the PostgreSQL engine for GitLab.com services, as well as a range of related systems which help to ensure the availability and reliability of the database engine.

### Practices

The [Practices Team](/handbook/engineering/infrastructure/team/reliability/practices.html) has [SRE's](/job-families/engineering/infrastructure/site-reliability-engineer/) that work full time directly with [Stage Groups Teams](/handbook/product/categories/) to focus on their Reliability and Infrastructure concerns for GitLab.com when their need is greater than the [General Team](/handbook/engineering/infrastructure/team/reliability/general.html) can support.

## How We Work

We maintain a single source of truth epic for all work underway for the Reliability team. That epic can be found at [GitLab SaaS Reliability - work queue](https://gitlab.com/groups/gitlab-com/gl-infra/-/epics/898) and represents the current state of project work assigned within teams. That epic references projects detailed in the form of sub-epics.

### General Workflow

1. GitLab team members open new issues in [Reliability Issue Tracker](https://gitlab.com/gitlab-com/gl-infra/reliability/-/issues)
2. New issues go through a [management and prioritization process](/handbook/engineering/infrastructure/team/reliability/issues.html)

### Sources of work

1. Issues generated, mostly by Reliability team members that are necessary to meet our OKRs.
1. Issues generated by GitLab team members outside of Reliability via one of the paths documented in the [Getting Assistance](#getting-assistance) section.
1. Issues generated as [`Corrective Actions`](/handbook/engineering/infrastructure/incident-management/#corrective-actions) for incidents
1. Issues generated as miscellaneous small tasks found in the day-to-day of an SRE/DBRE/EM

### OKRs

[OKRs for the Reliability team](https://gitlab.com/gitlab-com/gitlab-OKRs/-/issues/?label_name%5B%5D=Sub-Department%3A%3AReliability) that require status tracking should be updated each Wednesday.
When updating the progress percentage of any given KR, it is not necessary to provide extensive notes.
One sentence with a link to a larger update is sufficient in most cases.
It is also acceptable to do a check-in without providing a note on a single occasion, but not over several check-ins.

It is important that we set the following on every Reliability OKR:

1. The label `~Sub-Department::Reliability`.
1. An appropriate Reliability team label (e.g. `~Reliability::Practices`).
1. For work that is scheduled, the OKR should be assigned to [one of the quarterly milestone](https://gitlab.com/gitlab-com/gitlab-OKRs/-/milestones).

[Production Readiness]: /handbook/engineering/infrastructure/production/readiness/
