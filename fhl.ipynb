{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "134a0785",
   "metadata": {},
   "source": [
    "# FHL demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from chroma2 import Chroma2\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain.document_loaders import TextLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433363a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dir(path, glob):\n",
    "    \"\"\"Load documents.\"\"\"\n",
    "    p = Path(path)\n",
    "    docs = []\n",
    "    for i in p.glob(glob):\n",
    "        if i.is_file():\n",
    "            sub_docs = TextLoader(str(i)).load()\n",
    "            docs.extend(sub_docs)\n",
    "    return docs\n",
    "\n",
    "def chunks(xs, n):\n",
    "    n = max(1, n)\n",
    "    return (xs[i:i+n] for i in range(0, len(xs), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_handbook_all():\n",
    "    return load_dir(\"data/handbook/\", glob=\"**/*.md\")\n",
    "\n",
    "def load_handbook():\n",
    "    directories=[\"travel\",\"paid-time-off\",\"people-policies\", \"hiring\", \"incentives\", \"legal\"]\n",
    "    documents = []\n",
    "    for dir in directories:\n",
    "        documents += load_dir(f'data/handbook/{dir}/', glob=\"**/*.md\")\n",
    "    return documents\n",
    "def split_documents(documents):\n",
    "    #text_splitter = TokenTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "    text_splitter = SpacyTextSplitter.from_tiktoken_encoder(chunk_size=800, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "239475d2",
   "metadata": {},
   "source": [
    "### Initiatilize the vector store, also loading anything previous persisted under the \"collection_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8930cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma2(collection_name=\"handbook\",embedding_function=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d53d640",
   "metadata": {},
   "source": [
    "## Load and process the employee handbook corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d37b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "handbook_docs = load_handbook_all()\n",
    "splitted = split_documents(handbook_docs)\n",
    "splitted = [doc for doc in splitted if len(doc.page_content) < 8192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf190ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(splitted)\n",
    "\n",
    "if total < 1000:\n",
    "    vectorstore.add_documents(splitted)\n",
    "else: \n",
    "    import time\n",
    "    MAX_PER_MIN = 1500\n",
    "    BATCH_PER_MIN = 3\n",
    "    SPLIT_COUNT = MAX_PER_MIN / BATCH_PER_MIN\n",
    "    MIN_TIME_PER_BATCH = 60 / BATCH_PER_MIN\n",
    "\n",
    "    doc_chunks = chunks(splitted, int(SPLIT_COUNT))\n",
    "    count = 0\n",
    "    for docs in doc_chunks:\n",
    "        print(\"chunk\", len(docs))\n",
    "        start = time.perf_counter()\n",
    "        vectorstore.add_documents(documents=docs)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        count += len(docs)\n",
    "        if (count < total) and (elapsed < MIN_TIME_PER_BATCH):\n",
    "            print(\"sleeping for\", MIN_TIME_PER_BATCH - elapsed)\n",
    "            time.sleep(MIN_TIME_PER_BATCH - elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fc8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires patching duckdb as there's a parser error\n",
    "vectorstore._client._db.persist()\n",
    "#vectorstore._client._db.get_save_folder()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f86a8d75",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c28cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ai = OpenAI(temperature=0,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import markdown\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "template_hdr = \"\"\"Given a question and the following contexts (labeled as #0, #1, and #2), follow these instructions.\n",
    "First, see if the question can be answered solely by the information provided in the contexts. If it cannot, return this output:\n",
    "Answer: Not found\n",
    "\n",
    "Otherwise, Provide a verbose, comprehensive answer based solely on the information found in the provided contexts. Your output should of the form:\n",
    "    Context: <comma separated array containing names of all contexts referenced in the answer>\n",
    "    Answer: <answer to the question>\n",
    "\"\"\"\n",
    "\n",
    "def construct_prompt(query):\n",
    "  output = template_hdr\n",
    "  results = vectorstore.similarity_search(query,k=3)\n",
    "\n",
    "  for i in range(len(results)):\n",
    "    doc = results[i]\n",
    "    output += f\"\"\"=== CONTEXT #{i} ===\n",
    "    {doc.page_content}\n",
    "    ===\n",
    "    \"\"\"\n",
    "  output += f\"\"\"Question: {query}\"\"\"\n",
    "  return output, results\n",
    "\n",
    "def question(q):\n",
    "  prompt, source_results = construct_prompt(q)\n",
    "  response = ai(prompt)\n",
    "\n",
    "  # parse the response\n",
    "  context = \"\"\n",
    "  answer = \"\"\n",
    "  try:\n",
    "    results = re.search(r'CONTEXT:\\s*(.*?)\\s*ANSWER:\\s*(.*?)$', response, re.DOTALL|re.IGNORECASE)\n",
    "    context = results.group(1)\n",
    "    answer = results.group(2)\n",
    "\n",
    "    found_answer = answer.lower().strip != \"not found.\"\n",
    "\n",
    "    # parse the context\n",
    "    if found_answer:\n",
    "      sources = context.split(\",\")\n",
    "      sources = [element for element in sources if (not element.isspace() and element)]\n",
    "      sources = [re.search(r'([0-9])', src).group(1) for src in sources]\n",
    "      sources = [int(s) for s in sources]\n",
    "      sources = [source_results[i] for i in sources]\n",
    "    else:\n",
    "      sources = source_results   \n",
    "\n",
    "    output = f\"\"\"\n",
    "    <h2><div style=\"width:800px\">{answer}</div></h3><br>\n",
    "    <h4>\n",
    "    Sources:\n",
    "    \"\"\"\n",
    "\n",
    "    for source in sources:\n",
    "      output += f\"\"\"<div><h3><b>{source.metadata['source']}</b></h3></div>\n",
    "      <i><div style=\"background-color:#333;margin-bottom:32px\">...{markdown.markdown(source.page_content)}...</div></i>\n",
    "      \"\"\"\n",
    "    \n",
    "    display(HTML(output))\n",
    "  except Exception as e:\n",
    "    if (re.search(\"not found\", response, re.IGNORECASE)):\n",
    "      display(HTML(f\"\"\"<h2>No answer found</h2>\"\"\"))\n",
    "    else:\n",
    "      print(\"EXCEPTION!\", e)\n",
    "      print(prompt)\n",
    "      print(response)\n",
    "      print(context)\n",
    "      print(\"\\n\\n\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Leave policy\n",
    "#question(\"How much PTO do I get?\")\n",
    "#question(\"How do I file time off?\")\n",
    "\n",
    "# Travel policy\n",
    "#question(\"I'm very tall. What are my options for flights?\")\n",
    "#question(\"What's our policy re: Covid-19?\")\n",
    "\n",
    "# Unknown (not answered in DB)\n",
    "#question(\"Are chainsaws ok?\")\n",
    "#question(\"When do paychecks arrive?\")\n",
    "\n",
    "# Hiring\n",
    "#question(\"How do I submit someone for a referral?\")\n",
    "#question(\"Is there a referral bonus?\")\n",
    "\n",
    "# Legal\n",
    "#question(\"How do I create a NDA?\")\n",
    "#question(\"Is there an bonus awarded for getting a patent?\")\n",
    "\n",
    "# Promo\n",
    "# question(\"What dates are promotions finalized?\")\n",
    "# question(\"What are the levels in the career ladder?\")\n",
    "\n",
    "# Culture\n",
    "# question(\"What are the cultural values at Gitlab?\")\n",
    "\n",
    "# Problematic questions\n",
    "# question(\"Where should decisions made in a meeting be captured?\") # includes extraneous information board meetings.\n",
    "question(\"What happens when the NDA is rejected?\") # Nonsense answer.\n",
    "# question(\"What kind of NDA do we use during a separation agreement?\") # Nonsense answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c865e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4cb34ac04d2f63e8dd7a2b828fd0a9d1c30833be22708a543587de3f7cfd78e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
